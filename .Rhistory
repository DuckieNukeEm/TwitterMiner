m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
head(m)
as.character(myCorpus[[1]])
as.character(myCorpus[[2]])
as.character(myCorpus[[3]])
as.character(myCorpus[[4]])
as.character(myCorpus[[5]])
as.character(myCorpus[[6]])
tdm[1,1]
tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, try(tolower = TRUE , silent = TRUE)))
tweets.corpus = Corpus(VectorSource(tweets.text))
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, try(tolower = TRUE , silent = TRUE)))
#transfomes words and clean 'em up
clean_sentence <- function(sentence){
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
#removing icons
sentence = iconv(sentence, "latin1", "ASCII", sub="")
# and convert to lower case:
sentence = try(tolower(sentence), silent = T)
return(sentence)
}
tweets.text <- lapply(tweets.text, clean_sentence)
tweets.corpus = Corpus(VectorSource(tweets.text))
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = False))
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
tweets.tdm = as.matrix(tweets.tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
clean_sentence <- function(sentence, remove.words = NULL){
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
#removing icons
sentence = iconv(sentence, "latin1", "ASCII", sub="")
# and convert to lower case:
sentence = try(tolower(sentence), silent = T)
if(remove.words)
{sentence <- removeWords(sentence, remove.words)}
return(sentence)
}
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","WalMart","WM","wm","Wal-Mart"))
t<-NULL
length(t)
if(length(t)) {3}
t<-1
if(length(t)) {3}
clean_sentence <- function(sentence, remove.words = NULL){
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
#removing icons
sentence = iconv(sentence, "latin1", "ASCII", sub="")
# and convert to lower case:
sentence = try(tolower(sentence), silent = T)
if(length(remove.words)
{sentence <- removeWords(sentence, remove.words)}
return(sentence)
}
#transfomes words and clean 'em up
clean_sentence <- function(sentence, remove.words = NULL){
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
#removing icons
sentence = iconv(sentence, "latin1", "ASCII", sub="")
# and convert to lower case:
sentence = try(tolower(sentence), silent = T)
if(length(remove.words))
{sentence <- removeWords(sentence, remove.words)}
return(sentence)
}
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","WalMart","WM","wm","Wal-Mart"))
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart, "@Walmart","Walmart",WM","wm","Wal-Mart"))
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart"))
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
#transfomes words and clean 'em up
clean_sentence <- function(sentence, remove.words = NULL){
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
#removing icons
sentence = iconv(sentence, "latin1", "ASCII", sub="")
# and convert to lower case:
sentence = try(tolower(sentence), silent = T)
if(length(remove.words)) {sentence <- removeWords(sentence, remove.words)}
return(sentence)
}
tweets.text = lapply(tweets.text, removeWords, c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart"))
tweets.text[1]
tweets.text[2]
tweets.text[3]
tweets.text[4]
tweets.text[5]
tweets.corpus = Corpus(VectorSource(tweets.text))
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tweets.text = laply(tweets.tweets, function(t) t$getText() )
tweets.text[5]
tweets.text[1500]
tweets.text[150]
length(tweets.text)
tweets.text[1400]
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart"))
tweets.text[1400]
dm <- dm[-c(1,2)]
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tweets.tdm = as.matrix(tweets.tdm)
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart","Mart","mart"))
#In tm package, the documents are managed by a structure called Corpus
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
#verb_print(paste(c("Scrapped ", length(wm.tweets),". ",length(wm.tweets)/(1500*2), "% efficent" ), sep ="")
verb_print("Cleaning the tweets")
tweets.tweets <- strip_retweets(tweets.tweets,strip_manual=TRUE,strip_mt=TRUE)
verb_print("Converting tweets to text")
tweets.text = laply(tweets.tweets, function(t) t$getText() )
verb_print("Scoring Tweets")
tweets.score <- score.sentiment(tweets.text, pos_list, neg_list, .progress='text')
verb_print("histogram time")
hist(tweets.score$score)
verb_print("tweets to score now")
score2<- data.frame(score = as.integer(), text =as.character(), code = as.character())
for(i in tweetChar)
{ verb_print(i)
score2 <- rbind(score2, tweet_to_score(i, pos.word =  pos_list, neg.word = neg_list, .progress='text'))}
verb_print("trying a wordcloud thingy")
#cleaning sentence
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart","Mart","mart"))
#In tm package, the documents are managed by a structure called Corpus
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
score2<- data.frame(score = as.integer(), text =as.character(), code = as.character())
sapply(tweetChar, Function(x) score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text')))
sapply(tweetChar, Function(x) (score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text')))
)
sapply(tweetChar, function(x) (score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text'))))
sapply(tweetChar, function(x) (score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text'))), .progress='text')
sapply(tweetChar, function(x) (print(x) ; score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text'))))
sapply(tweetChar, function(x) (print(x) score2 <- rbind(score2, tweet_to_score(x, pos.word =  pos_list, neg.word = neg_list, .progress='text'))))
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
#verb_print(paste(c("Scrapped ", length(wm.tweets),". ",length(wm.tweets)/(1500*2), "% efficent" ), sep ="")
verb_print("Cleaning the tweets")
tweets.tweets <- strip_retweets(tweets.tweets,strip_manual=TRUE,strip_mt=TRUE)
verb_print("Converting tweets to text")
tweets.text = laply(tweets.tweets, function(t) t$getText() )
verb_print("Scoring Tweets")
tweets.score <- score.sentiment(tweets.text, pos_list, neg_list, .progress='text')
verb_print("histogram time")
hist(tweets.score$score)
verb_print("tweets to score now")
score2<- data.frame(score = as.integer(), text =as.character(), code = as.character())
for(i in tweetChar)
{ verb_print(i)
score2 <- rbind(score2, tweet_to_score(i, pos.word =  pos_list, neg.word = neg_list, .progress='text'))}
verb_print("trying a wordcloud thingy")
#cleaning sentence
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart","Mart","mart"))
#In tm package, the documents are managed by a structure called Corpus
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart","#wm","#WalMart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@Walmart", "@wal-mart","#WalMart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart") #,"#wm","#WalMart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart") #,"#wm","#WalMart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart","#wm","#WalMart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
#verb_print(paste(c("Scrapped ", length(wm.tweets),". ",length(wm.tweets)/(1500*2), "% efficent" ), sep ="")
verb_print("Cleaning the tweets")
tweets.tweets <- strip_retweets(tweets.tweets,strip_manual=TRUE,strip_mt=TRUE)
verb_print("Converting tweets to text")
tweets.text = laply(tweets.tweets, function(t) t$getText() )
verb_print("Scoring Tweets")
tweets.score <- score.sentiment(tweets.text, pos_list, neg_list, .progress='text')
verb_print("histogram time")
hist(tweets.score$score)
verb_print("tweets to score now")
score2<- data.frame(score = as.integer(), text =as.character(), code = as.character())
for(i in tweetChar)
{ verb_print(i)
score2 <- rbind(score2, tweet_to_score(i, pos.word =  pos_list, neg.word = neg_list, .progress='text'))}
verb_print("trying a wordcloud thingy")
#cleaning sentence
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart","Mart","mart"))
#In tm package, the documents are managed by a structure called Corpus
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# Authenticatng
#do it yourself
# Setting the working directory
wd <- getwd()
wdcode <- file.path(wd, 'R')
wddata <- file.path(wd, 'Data')
# activating the librarys
pkg <- c("twitteR", "plyr","stringr","tm","wordcloud","RColorBrewer","ggplot2")
inst <- pkg %in% installed.packages()
if(length(pkg[!inst]) > 0) install.packages(pkg[!inst])
lapply(pkg,library,character.only=TRUE)
rm(inst,pkg)
# Setting control Variables
verbose = TRUE
TweetCounts = 1500
tweets.tweets <- list()
tweetChar <- c("@wm","@Walmart", "@wal-mart")
# Loading the R functions, switching to verb_print to define sections
if(verbose){print("Loading R functions")}
source(file.path(wdcode, 'TwitterFunctions.R') )
verb_print("Loading the data sources now")
pos_list = scan(file.path(wddata, "hu_positive_words.txt"), what='character', comment.char=';')
neg_list = scan(file.path(wddata, "hu_negative_words.txt"), what='character', comment.char=';')
verb_print("scrapping R now")
for(i in tweetChar)
{ verb_print(i)
tweets.tweets <- c(tweets.tweets, searchTwitter(i, n=TweetCounts))}
#verb_print(paste(c("Scrapped ", length(wm.tweets),". ",length(wm.tweets)/(1500*2), "% efficent" ), sep ="")
verb_print("Cleaning the tweets")
tweets.tweets <- strip_retweets(tweets.tweets,strip_manual=TRUE,strip_mt=TRUE)
verb_print("Converting tweets to text")
tweets.text = laply(tweets.tweets, function(t) t$getText() )
verb_print("Scoring Tweets")
tweets.score <- score.sentiment(tweets.text, pos_list, neg_list, .progress='text')
verb_print("histogram time")
hist(tweets.score$score)
verb_print("tweets to score now")
score2<- data.frame(score = as.integer(), text =as.character(), code = as.character())
for(i in tweetChar)
{ verb_print(i)
score2 <- rbind(score2, tweet_to_score(i, pos.word =  pos_list, neg.word = neg_list, .progress='text'))}
verb_print("trying a wordcloud thingy")
#cleaning sentence
tweets.text <- lapply(tweets.text, clean_sentence, remove.words = c("walmart","wal","@WalMart","WalMart", "@Walmart","Walmart","WM","wm","Wal-Mart","Mart","mart"))
#In tm package, the documents are managed by a structure called Corpus
tweets.corpus = Corpus(VectorSource(tweets.text))
#Create a term-document matrix from a corpus
tweets.tdm = TermDocumentMatrix(tweets.corpus,control = list(removePunctuation = TRUE,stopwords = c("WM", "walmart","WalMart","Wal-Mart","WalMart", stopwords("english")), removeNumbers = TRUE, tolower = F))
#Convert as matrix
tweets.tdm = as.matrix(tweets.tdm)
#Get word counts in decreasing order
tweets.word_freqs = sort(rowSums(tweets.tdm), decreasing=TRUE)
#Create data frame with words and their frequencies
dm = data.frame(word=names(tweets.word_freqs), freq=tweets.word_freqs)
#Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
hist(tweets.score$score)
hist(score2$score, by score2$code)
hist(score2$score)
hist(score2$score)
